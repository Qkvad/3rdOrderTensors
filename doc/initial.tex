%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[twoside]{article}

\usepackage[sc]{mathpazo} % Use the Palatino font
\usepackage[T1]{fontenc} % Use 8-bit encoding that has 256 glyphs
\linespread{1.05} % Line spacing - Palatino needs more space between lines
\usepackage{microtype} % Slightly tweak font spacing for aesthetics

\usepackage[english]{babel} % Language hyphenation and typographical rules

\usepackage[hmarginratio=1:1,top=32mm,columnsep=20pt]{geometry} % Document margins
\usepackage[hang, small,labelfont=bf,up,textfont=it,up]{caption} % Custom captions under/above floats in tables or figures
\usepackage{booktabs} % Horizontal rules in tables

\usepackage{lettrine} % The lettrine is the first enlarged letter at the beginning of the text

\usepackage{enumitem} % Customized lists
\setlist[itemize]{noitemsep} % Make itemize lists more compact

\usepackage{titlesec} % Allows customization of titles
\renewcommand\thesection{\Roman{section}} % Roman numerals for the sections
\renewcommand\thesubsection{\roman{subsection}} % roman numerals for subsections
\titleformat{\section}[block]{\large\scshape\centering}{\thesection.}{1em}{} % Change the look of the section titles
\titleformat{\subsection}[block]{\large}{\thesubsection.}{1em}{} % Change the look of the section titles

\usepackage{fancyhdr} % Headers and footers
\pagestyle{fancy} % All pages have headers and footers
\fancyhead{} % Blank out the default header
\fancyfoot{} % Blank out the default footer
\fancyhead[C]{Title $\bullet$ May 2018} % Custom header text
\fancyfoot[RO,LE]{\thepage} % Custom footer text

\usepackage{titling} % Customizing the title section

\usepackage{hyperref} % For hyperlinks in the PDF

\usepackage{multicol}
\usepackage{graphicx}
\graphicspath{{img/}}
\usepackage{float}

\usepackage{amssymb,amsmath,amsthm}
\newtheorem{theorem}{Theorem}
\newtheorem{defn}{Definition}

\newcommand{\code}[1]{\texttt{#1}}

%----------------------------------------------------------------------------------------
%	TITLE SECTION
%----------------------------------------------------------------------------------------

\setlength{\droptitle}{-4\baselineskip} % Move the title up

\pretitle{\begin{center}\Huge\bfseries} % Article title formatting
\posttitle{\end{center}} % Article title closing formatting
\title{Title} % Article title

\author{%
\textsc{Ivan \v{C}eh}\\% \\[1ex] 
\normalsize Computer Science \\
\normalsize \href{mailto:cehmail@gmail.com}{cehmail@gmail.com} 
\and
\textsc{Petra Br\v{c}i\'c}\\% \\[1ex] 
\normalsize Applied Mathematics \\
\normalsize \href{mailto:petrabrcic94@gmail.com}{petrabrcic94@gmail.com} 
\and
\textsc{Sandro Lovni\v{c}ki}\\% \\[1ex] 
\normalsize Computer Science \\ 
\normalsize \href{mailto:lovnicki.sandro@gmail.com}{lovnicki.sandro@gmail.com}
}

\date{\today} 
\renewcommand{\maketitlehookd}{%
\noindent \textbf{\\Abstract:} Operations with tensors, or multiway arrays, have become increasingly prevalent in recent years. Traditionally, tensors are represented or decomposed as a sum of rank-1 outer products, but in this paper we explore an alternate representation of tensors which shows promise with respect to the tensor approximation problem. We also discuss implications for extending basic algorithms such as the power method, QR iteration, and Krylov subspace methods. To conclude, we present two applications of this theoretical framework; image deblurring and face recognition.
~
\\\\
\textbf{Keywords:} tensor, deblurring, face recognition.
}


%----------------------------------------------------------------------------------------

\begin{document}

% Print the title and table of contents
\maketitle
%\tableofcontents
%\newpage

%----------------------------------------------------------------------------------------
%	ARTICLE CONTENTS
%----------------------------------------------------------------------------------------
\begin{multicols}{2}

\section{Introduction}
A tensor is a multi-dimensional array of numbers. For example, we could say that vector $v \in \mathbb{R}^{n_1}$ is a first-order tensor and similarily, matrix $M \in \mathbb{R}^{n_1 \times n_2}$ is a second-order tensor. Therefore, a third-order tensor is $\mathcal{A} \in \mathbb{R}^{n_1 \times n_2 \times n_3}$.\\
\indent In this paper, we provide a setting in which the familiar tools of linear algebra can be extended to better understand third-order tensors. Significant contributions to extending matrix analysis has been made in the works of Misha E. Kilmer and others in \cite{kilmer-braman-hao} and \cite{kilmer-martin}.\\
\indent In sections that follow, we first introduce the basic notions and definitions of this new framework. We present methods which will be used in applictions and continue with two examples (image deblurring and face recognition), showing our implementation in Octave and human-readable results.

\section{Theoretical Framework}
In this section, we will first describe notation that is convenient for manipulating third-order tensors and which will be used throughout the paper. Also, we define basic operations on tensors such as multiplication,...

\subsection{Notation and Conversions}
Each third-order tensor $\mathcal{A} \in \mathbb{R}^{l \times m \times n}$ can be viewed as $n$ $l \times m$ matrices stacked upon eachother. Firthermore, it can also be viewed as a $l \times m$ matrix of vectors of length $n$. It is convenient to break tensor into "slices" and "tubal elements".
\begin{defn} Let $\mathcal{A} \in \mathbb{R}^{l \times m \times n}$. We introduce the following notation:
\begin{itemize}
	\item $i$-th \textbf{frontal slice} $\mathcal{A}^{(i)} \equiv \mathcal{A}(:,:,i)$
	\item $i$-th \textbf{lateral slice} $\overrightarrow{\mathcal{A}}_i \equiv \mathcal{A}(:,i,:)$
	\item $i,k$-th \textbf{tubal scalar} $\textbf{a}_{ik} \equiv \mathcal{A}(i,k,:)$
\end{itemize}
\end{defn}

In order to define tensor multiplication, we now present some conversions between tensor and matrix objects. To wrap a tensor into a matrix, functions \code{circ} and \code{matvec} will be used, and to unwrap a tensor from the return value of \code{matvec}, we use \code{foldn} operator. How those operators work on tensors and matrices is now presented.
\[
\code{circ}(\mathcal{A}) =
  \begin{bmatrix}
    \mathcal{A}^{(1)} & \mathcal{A}^{(n)} & \mathcal{A}^{(n-1)} & \dots & \mathcal{A}^{(2)} \\
    \mathcal{A}^{(2)} & \mathcal{A}^{(1)} & \mathcal{A}^{(n)}   &  \dots  & \mathcal{A}^{(3)} \\
    \vdots            & \vdots            & \vdots              &  \ddots & \vdots \\
    \mathcal{A}^{(n)} & \mathcal{A}^{(n-1)} & \mathcal{A}^{(n-2)}   &  \dots & \mathcal{A}^{(1)} 
  \end{bmatrix}
\]

\[
\code{matvec}(\mathcal{A}) =
  \begin{bmatrix}
    \mathcal{A}^{(1)} \\
    \mathcal{A}^{(2)} \\
    \vdots            \\
    \mathcal{A}^{(n)} 
  \end{bmatrix}
\]

\noindent Correspondingly, \code{foldn} operator is defined so that \code{foldn}(\code{matvec}($\mathcal{A}),n$) = $\mathcal{A}$.

\subsection{Tensor Operators}
Now, with established notation and conversions, we are equipped to define needed tensor operators.

\begin{defn} Let $\mathcal{A} \in \mathbb{R}^{l \times p \times n}$ and $\mathcal{B} \in \mathbb{R}^{p \times m \times n}$. The t-product\footnote{our Octave implementation of t-product is called \code{tproduct}.} $\mathcal{A} \ast \mathcal{B}$ is a $l \times m \times n$ tensor defined as
\[ \mathcal{A} \ast \mathcal{B} := \code{foldn}(\code{circ}(\mathcal{A}) \cdot \code{matvec}(\mathcal{B}) ,n) \]
\end{defn}

\begin{defn} If $\mathcal{A} \in \mathbb{R}^{l \times m \times n}$, then $\mathcal{A}^T \in \mathbb{R}^{m \times l \times n}$ is obtained by transposing each frontal slice and reversing the order of frontal slices $2$ to $n$.
\end{defn}

It could be observed that $m \times 1 \times n$ tensors are just matrices oriented laterally. Furthermore, we could twist a $m \times n$ matrix to make a $m \times 1 \times n$ tensor. Following this line of thought, we introduce operators \code{twist} and \code{squeeze} defined as
\[ \code{squeeze}(\overrightarrow{\mathcal{A}}) = A \in \mathbb{R}^{m \times n} \]
\[ \code{twist}(\code{squeeze}(\overrightarrow{\mathcal{A}})) := \overrightarrow{\mathcal{A}} \]

\noindent Observing the established setting, it is useful to introduce $\mathbb{K}^m_n$ - the set of all $m \times n$ matrices oriented as $m \times 1 \times n$ tensors. For convenience, we write just $\mathbb{K}_n$ when $m=1$ and use it to represent the set of all tubal scalars of length $n$. As we mentioned before, $l \times m \times n$ tensor can be viewed as a $l \times m$ matrix of tubal scalars, so it is natural to say that $K^{l \times m}_n$ is the set of all $l \times m \times n$ tensors, i.e. $K^{l \times m}_n \equiv \mathbb{R}^{l \times m \times n}$.

This new definitions have as a consequence the consistency of operations on $\mathbb{K}^{l \times m}_n$ which we stress in the following list:
\begin{enumerate}
	\item multiplication, factorization, etc. based on t-product reduce to the standard matrix operations and factorizations when $n=1$.
	\item outer-products of matrices are well-defined.
	\item given $\overrightarrow{\mathcal{X}}, \overrightarrow{\mathcal{Y}} \in \mathbb{K}^m_n$, $\overrightarrow{\mathcal{X}}^T \ast \overrightarrow{\mathcal{Y}}$ is a scalar.
\end{enumerate} 
%------------------------------------------------

\section{Image Deblurring}
In this section...

\subsection{Subsection}
Based on...


\section{Face Recognition}
As we could have seen in...


%------------------------------------------------


%----------------------------------------------------------------------------------------
%	REFERENCE LIST
%----------------------------------------------------------------------------------------

\begin{thebibliography}{99}

\bibitem[1]{kilmer-braman-hao}
Misha E. Kilmer, Karen Braman, Ning Hao.
\newblock Third Order Tensors as Operators on Matrices: A
Theoretical and Computational Framework with
Applications in Imaging

\bibitem[2]{kilmer-martin}
Misha E. Kilmer, Carls D. Martin
\newblock Factorization Strategies for Third-order Tensors
 
\end{thebibliography}

%----------------------------------------------------------------------------------------

\end{multicols}{2}
\end{document}
